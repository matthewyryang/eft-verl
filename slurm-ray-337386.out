--- DEBUGGING RAY JOB ENVIRONMENT ---
Python executable: /work/10913/myang13/miniconda3/bin/python
Torch version: 2.8.0+cpu
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Thu_Mar_28_02:24:28_PDT_2024
Cuda compilation tools, release 12.4, V12.4.131
Build cuda_12.4.r12.4/compiler.34097967_0
LD_LIBRARY_PATH is: /work/10913/myang13/miniconda3/envs/vllm/lib:/work/10913/myang13/miniconda3/lib:/usr/local/cuda-12.4/lib64:/usr/local/cuda-12.4/targets/aarch64-linux/lib:/opt/apps/cuda/12.4/extras/CUPTI/lib64:/opt/apps/cuda/12.4/lib64:/work/10913/myang13/miniconda3/envs/vllm/lib:/work/10913/myang13/miniconda3/lib:/usr/local/cuda-12.4/lib64:/usr/local/cuda-12.4/targets/aarch64-linux/lib:/opt/apps/nvidia24/openmpi/5.0.5/lib:/opt/apps/ucx/1.18.1/lib:/opt/apps/ucc/1.4.4/lib:/home1/apps/nvidia/Linux_aarch64/24.7/math_libs/nvpl/lib:/home1/apps/nvidia/Linux_aarch64/24.7/compilers/extras/qd/lib:/home1/apps/nvidia/Linux_aarch64/24.7/compilers/lib
real-time non-blocking time  (microseconds, -R) unlimited
core file size              (blocks, -c) 0
data seg size               (kbytes, -d) unlimited
scheduling priority                 (-e) 0
file size                   (blocks, -f) unlimited
pending signals                     (-i) 120525
max locked memory           (kbytes, -l) unlimited
max memory size             (kbytes, -m) unlimited
open files                          (-n) 256000
pipe size                (512 bytes, -p) 8
POSIX message queues         (bytes, -q) 819200
real-time priority                  (-r) 0
stack size                  (kbytes, -s) unlimited
cpu time                   (seconds, -t) unlimited
max user processes                  (-u) 16384
virtual memory              (kbytes, -v) unlimited
file locks                          (-x) unlimited
-----------------------------------
Running on nodes: c622-[112,121-122,131]
Job ID: 337386
--------------------
Head Node: c622-112
Head Node IP: 129.114.18.55
Worker Nodes: c622-121 c622-122 c622-131
--------------------
Starting Ray head node on c622-112...
Ray head node PID: 1324541
2025-08-22 14:47:50,890	INFO usage_lib.py:467 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2025-08-22 14:47:50,890	INFO scripts.py:913 -- [37mLocal node IP[39m: [1m129.114.18.55[22m
2025-08-22 14:47:55,440	SUCC scripts.py:949 -- [32m--------------------[39m
2025-08-22 14:47:55,440	SUCC scripts.py:950 -- [32mRay runtime started.[39m
2025-08-22 14:47:55,440	SUCC scripts.py:951 -- [32m--------------------[39m
2025-08-22 14:47:55,440	INFO scripts.py:953 -- [36mNext steps[39m
2025-08-22 14:47:55,440	INFO scripts.py:956 -- To add another node to this Ray cluster, run
2025-08-22 14:47:55,440	INFO scripts.py:959 -- [1m  ray start --address='129.114.18.55:6379'[22m
2025-08-22 14:47:55,440	INFO scripts.py:968 -- To connect to this Ray cluster:
2025-08-22 14:47:55,441	INFO scripts.py:970 -- [35mimport[39m[26m ray
2025-08-22 14:47:55,441	INFO scripts.py:971 -- ray[35m.[39m[26minit(_node_ip_address[35m=[39m[26m[33m'129.114.18.55'[39m[26m)
2025-08-22 14:47:55,441	INFO scripts.py:983 -- To submit a Ray job using the Ray Jobs CLI:
2025-08-22 14:47:55,441	INFO scripts.py:984 -- [1m  RAY_ADDRESS='http://129.114.18.55:8265' ray job submit --working-dir . -- python my_script.py[22m
2025-08-22 14:47:55,441	INFO scripts.py:993 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html 
2025-08-22 14:47:55,441	INFO scripts.py:997 -- for more information on submitting Ray jobs to the Ray cluster.
2025-08-22 14:47:55,441	INFO scripts.py:1002 -- To terminate the Ray runtime, run
2025-08-22 14:47:55,441	INFO scripts.py:1003 -- [1m  ray stop[22m
2025-08-22 14:47:55,441	INFO scripts.py:1006 -- To view the status of the cluster, use
2025-08-22 14:47:55,441	INFO scripts.py:1007 --   [1mray status[22m[26m
2025-08-22 14:47:55,441	INFO scripts.py:1011 -- To monitor and debug Ray, view the dashboard at 
2025-08-22 14:47:55,441	INFO scripts.py:1012 --   [1m129.114.18.55:8265[22m[26m
2025-08-22 14:47:55,441	INFO scripts.py:1019 -- [4mIf connection to the dashboard fails, check your firewall settings and network configuration.[24m
2025-08-22 14:47:55,441	INFO scripts.py:1123 -- [36m[1m--block[22m[39m
2025-08-22 14:47:55,441	INFO scripts.py:1124 -- This command will now block forever until terminated by a signal.
2025-08-22 14:47:55,441	INFO scripts.py:1127 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
Starting Ray worker nodes...
Starting worker on c622-121
Starting worker on c622-122
Starting worker on c622-131
Ray worker PIDs: 1325464 1325465 1325466
Waiting for cluster to form completely...
2025-08-22 14:48:00,220	INFO scripts.py:1094 -- [37mLocal node IP[39m: [1m129.114.18.58[22m
2025-08-22 14:48:01,481	SUCC scripts.py:1110 -- [32m--------------------[39m
2025-08-22 14:48:01,482	SUCC scripts.py:1111 -- [32mRay runtime started.[39m
2025-08-22 14:48:01,482	SUCC scripts.py:1112 -- [32m--------------------[39m
2025-08-22 14:48:01,482	INFO scripts.py:1114 -- To terminate the Ray runtime, run
2025-08-22 14:48:01,482	INFO scripts.py:1115 -- [1m  ray stop[22m
2025-08-22 14:48:01,482	INFO scripts.py:1123 -- [36m[1m--block[22m[39m
2025-08-22 14:48:01,482	INFO scripts.py:1124 -- This command will now block forever until terminated by a signal.
2025-08-22 14:48:01,482	INFO scripts.py:1127 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-08-22 14:48:00,283	INFO scripts.py:1094 -- [37mLocal node IP[39m: [1m129.114.18.57[22m
2025-08-22 14:48:01,543	SUCC scripts.py:1110 -- [32m--------------------[39m
2025-08-22 14:48:01,543	SUCC scripts.py:1111 -- [32mRay runtime started.[39m
2025-08-22 14:48:01,543	SUCC scripts.py:1112 -- [32m--------------------[39m
2025-08-22 14:48:01,543	INFO scripts.py:1114 -- To terminate the Ray runtime, run
2025-08-22 14:48:01,543	INFO scripts.py:1115 -- [1m  ray stop[22m
2025-08-22 14:48:01,543	INFO scripts.py:1123 -- [36m[1m--block[22m[39m
2025-08-22 14:48:01,543	INFO scripts.py:1124 -- This command will now block forever until terminated by a signal.
2025-08-22 14:48:01,543	INFO scripts.py:1127 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-08-22 14:48:00,284	INFO scripts.py:1094 -- [37mLocal node IP[39m: [1m129.114.18.56[22m
2025-08-22 14:48:01,560	SUCC scripts.py:1110 -- [32m--------------------[39m
2025-08-22 14:48:01,560	SUCC scripts.py:1111 -- [32mRay runtime started.[39m
2025-08-22 14:48:01,560	SUCC scripts.py:1112 -- [32m--------------------[39m
2025-08-22 14:48:01,561	INFO scripts.py:1114 -- To terminate the Ray runtime, run
2025-08-22 14:48:01,561	INFO scripts.py:1115 -- [1m  ray stop[22m
2025-08-22 14:48:01,561	INFO scripts.py:1123 -- [36m[1m--block[22m[39m
2025-08-22 14:48:01,561	INFO scripts.py:1124 -- This command will now block forever until terminated by a signal.
2025-08-22 14:48:01,561	INFO scripts.py:1127 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
Checking Ray cluster status...
======== Autoscaler status: 2025-08-22 14:48:16.674729 ========
Node status
---------------------------------------------------------------
Active:
 1 node_558000b0205f39582458bd8142966a8689c0cfbf219409fcb8de4243
 1 node_ca4d0e8c70bf1b9eaf1bd35a3e42b3fd3e1ff79a6c3832a7384acde7
 1 node_bf6c2320590c4858e418cb368f1f514433d580e0094468ea9fc774ba
 1 node_213011cc8b09b0df59885799df7eaebc2d39f62792dfba5098fdb878
Pending:
 (no pending nodes)
Recent failures:
 (no failures)

Resources
---------------------------------------------------------------
Total Usage:
 0.0/288.0 CPU
 0.0/4.0 GPU
 0B/598.18GiB memory
 0B/223.73GiB object_store_memory

Total Constraints:
 (no request_resources() constraints)
Total Demands:
 (no resource demands)
Submitting Ray job: /home1/10913/myang13/verl/eft/grpo/grpo_16k.sh from /home1/10913/myang13/verl
2025-08-22 14:48:24,146	INFO cli.py:41 -- [37mJob submission server address[39m: [1mhttp://129.114.18.55:8265[22m
2025-08-22 14:48:28,220	SUCC cli.py:65 -- [32m-------------------------------------------------------[39m
2025-08-22 14:48:28,220	SUCC cli.py:66 -- [32mJob 'raysubmit_CsmwicwMK3an3ssT' submitted successfully[39m
2025-08-22 14:48:28,220	SUCC cli.py:67 -- [32m-------------------------------------------------------[39m
2025-08-22 14:48:28,220	INFO cli.py:291 -- [36mNext steps[39m
2025-08-22 14:48:28,221	INFO cli.py:292 -- Query the logs of the job:
2025-08-22 14:48:28,221	INFO cli.py:294 -- [1mray job logs raysubmit_CsmwicwMK3an3ssT[22m
2025-08-22 14:48:28,221	INFO cli.py:296 -- Query the status of the job:
2025-08-22 14:48:28,221	INFO cli.py:298 -- [1mray job status raysubmit_CsmwicwMK3an3ssT[22m
2025-08-22 14:48:28,221	INFO cli.py:300 -- Request the job to be stopped:
2025-08-22 14:48:28,221	INFO cli.py:302 -- [1mray job stop raysubmit_CsmwicwMK3an3ssT[22m
Ray job submission command executed successfully (exit status 0).
Use 'ray job list' or the dashboard http://129.114.18.55:8265 to check job status.
Ray cluster is running.
Slurm job will remain active until Ray processes finish or time limit is reached.
